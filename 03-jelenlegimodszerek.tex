%----------------------------------------------------------------------------
\chapter{Jelenlegi módszerei}
%----------------------------------------------------------------------------

Az előző fejezetben leírt Bayes-hálók csomópontjai folytonos és diszkrét valószínűségi változók is lehetnek. Az automatizált struktúra tanulás során azonban az élek feltételes valószínűségi táblákként adódnak, ezek az algoritmusok csak diszkrét valószínűségi változókkal tudnak dolgozni. Ezért szükséges, hogy a bemeneti folytonos változók a struktúra tanulás előtt diszkretizálva legyenek.

Ez a fejezet bemutatja a diszkretizáció jelenlegi statisztikai módszereit. Példákat mutat az egyszerűbbek használatára és működésére. Emellett a jelenlegi Bayes-háló struktúra tanulási módszereket is leírja.

\section{Egyváltozós diszkretizáció}
A diszkretizációhoz az egyszerűbb megoldás, hogy minden változó a többitől függetlenül kerül diszkretizálásra. Az adatsor változatlansága esetén a változó diszkretizációja konstans marad.

A diszkretizációs intervallumok számának meghatározására több lehetőség van. Meghatározható egy konstans. Ezt szakértőre kell bízni, vagy egy viszonylag kicsi - de legalább 2 - számot választani, amihez emberi beavatkozásra van szükség. Dinamikusan is meghatározható. Ilyenkor az eredeti adatsor diszkrét változói közül a legnagyobb kardinalitású kiválasztható. Ezt a számosság értéket választjuk az intervallumok számának.

\begin{table}[h]\centering
    \begin{tabular}{ccc}
    A & B    & C    \\ \hline
    2 & 4.28 & 0.51 \\
    1 & 2.58 & 0.95 \\
    0 & 3.37 & 0.35 \\
    0 & 3.06 & 0.32 \\
    1 & 3.67 & 1.59 \\
    2 & 3.70 & 1.27 \\
    2 & 2.78 & 1.32 \\
    1 & 4.23 & 0.52 \\
    2 & 4.80 & 0.71 \\
    2 & 1.90 & 1.88
    \end{tabular}
    \caption{Véletlenszerűen generált adatpontok diszkretizációs példák bemutatásához}
    \label{tab:diszkretizacio-pelda}
\end{table}

Tekintsük a \ref{tab:diszkretizacio-pelda}. táblázat adatait. Ebben az esetben az \emph{A} változó diszkrét a \emph{B} és a \emph{C} változók folytonosak. A diszkrét változó értékei $\left\{ 0, 1, 2 \right\}$, így a kardinalitása \emph{3}. Ezért a példákban három intervallumra lesznek a folytonos változók osztva.

Az intervallumok többféle módon is megadhatók. Szövegesen leírható például így: a \emph{C} változó adatsorát az 1-nél nagyobb és nemnagyobb értékekre bontjuk. Ez matematikai jelölésekkel felírható a $\Lambda_{C} = \left\{ \interval[open left]{-\inf}{1}, \interval[open]{1}{\inf} \right\} $ módon, így $\Lambda$ jelöli az adatsor összes diszkretizációs intervallumának halmazát, az alsó index mutatja, mely változóhoz tartozik. Mivel minden pontot valamelyik intervallum tartalmazni fogja, ezért egy listában növekvő sorrendben elég megadni a határokat. Ráadásul a minimum és maximum érték is ismert, így ezek használhatók a $\inf$ szimbólum helyett vagy el is hagyhatók. Ezen a módon a $\Lambda_{C} = \langle 0.32, 1, 1.88 \rangle$. Ez a diszkretizációs sorozat \cite{friedman1996discretizing}. Függvény formában is leírható, ilyenkor
$f_{\Lambda_{C}}(x) = \left\{ \begin{array}{ll}
    0 & \mbox{ha } x \leq 1 \\
    1 & \mbox{ga } x > 1
\end{array} \right.$. Növekvő sorrendbe rendezett értékek esetén az intervallum utolsó elemének pozíciója is egyértelműen megadja a határt. A példában 6 db 1-nél nemnagyobb érték van, így $e_{C} = \langle 6 \rangle$.

\subsection{Egyenlő hosszú intervallumok}
A vizsgált változó értékkészletének tartománya felosztható az elvárt kardinalitásnak megfelelő, egyenlő hosszúságú intervallumra. Ennek kiszámolásához csak egy maximum és minimum keresésre van szükség, így rendkívül gyors és egyszerű. A diszkretizált értékek ilyenkor az eredeti értékek hisztogramján mutatják, hogy az adott érték melyik vödörbe került. Kiugró értékek esetén előfordulhat, hogy az egyes intervallumokba nagyon különböző számú adatpont esik, így a sok adatpontot tartalmazó intervallumokban az adatpontok közötti eltérések könnyen elvesznek, nem tanulható meg a Bayes-háló által. Ezért ezt a típusú felosztást egyenletes eloszlást közelítő adatsorokon érdemes használni, hogy ne legyen kevés adatpontot tartalmazó intervallum.

A példa esetében a legnagyobb folytonos értékek $\max \mathcal{X}_{B} = 4.8, \max \mathcal{X}_{C} = 1.88$, a legkisebb értékek pedig $\min \mathcal{X}_{B} = 1.9, \min \mathcal{X}_{C} = 0.32$. Három intervallumra kell osztani az értékeket, ezt jelölje $L=3$. Az \emph{i.} határ egyszerűen megkapható ($1 \leq i < L$):
$$ \Lambda_{V, i} = \frac{\max \mathcal{X}_{V} - \min \mathcal{X}_{V}}{L} \cdot i + \min \mathcal{X}_{V} $$
Így a példában a határok $\Lambda_{B} = \langle 2.92, 3.69 \rangle$ és  $\Lambda_{C} = \langle 0.84, 1.36 \rangle$.

\subsection{Egyenlő mintaszámú intervallumok}
A minták növekvő sorrendjében minden $N / L$. pozícióban található érték számít az intervallum határának. Ha az így számolt pozíció nem lenne egész szám, a kapott értéket a kerekítés szabályai szerint egész számra kerekítve választjuk ki a pozíciót. Ezeken a helyeken szereplő értékek adják az intervallumok határát. Hisztogramról leolvasható, mert L felbontású hisztogramon az adatponthoz az azt tartalmazó vödör sorszáma rendelhető. Egyenlő kvantilisű vagy egyenlő frekvenciájú intervallumoknak is nevezhetők.

A példában 10 minta szerepel, ezeket kell 3 intervallumban elhelyezni. Ezért az adott változó szerint sorbarendezett mintákban a 3.33 és 6.67 pozícióban levő értéket keressük. Tehát a 3. valamint 7. helyen levő értékek lesznek az intervallumok felső határai. Esetünkben így ezek a határok $\Lambda_{B} = \langle 2.78, 3.70 \rangle$ és  $\Lambda_{C} = \langle 0.51, 1.27 \rangle$.

\subsection{Nyomaték párosítás}
A diszkretizált értékeket $\langle d_{1}, d_{2}, \dots, d_{L-1}\rangle$ formában keressük, melynek a diszkrét eloszlását a $\langle p_{1}, p_{2}, \dots, p_{L-1}\rangle$ valószínűségek adják meg, és teljesítik a következő feltételeket \cite{nojavan2017comparative}:
\begin{align}
    p_{1}d_{1} + p_{2}d_{2} + \dots + p_{L-1}d_{L-1} &= q x_{1} + q x_{2} + \dots + q x_{N} = m \\
    p_{1}(d_{1} - m)^{2} + \dots + p_{L-1}(d_{L-1} - m)^{2} &= q(x_{1} - m)^{2} + \dots + q(x_{N} - m)^{2} \\
    p_{1}(d_{1} - m)^{2L-1} + \dots + p_{L-1}(d_{L-1} - m)^{2L-1} &= q(x_{1} - m)^{2L-1} + \dots + q(x_{N} - m)^{2L-1} \\
    p_{1} + p_{2} + \dots + p_{L-1} &= 1 \\
    q &= \frac{1}{N}
\end{align}
ahol \emph{x} a folytonos változó értékei, \emph{N} a számosságuk. Az \emph{m} került bevezetésre az adatsor átlagának jelölésére, a \emph{q} pedig az adatsort normalizáló tényező.

A diszkretizáció elvégzése után az új eloszlás minden rendbeli nyomatéka a megegyezik az eredeti adatsor adott nyomatékával. A diszkrét értékek számának növelésével növekszik az egyenletek száma és komplexitása. Az egyenletrendszert numerikusan is nehéz megoldani, mert nem konvex probléma, így az összes lehetséges értéken kell keresést végezni. A lehetséges értékek pedig a $ \mathbb{R}^{L-1} $ halmazban vannak. Lehetséges csak egy bizonyos pontosságig végezni a keresést és a minimum és maximum közé szorítani, így véges számú értéket kell kipróbálni. Ám az intervallumok számával exponenciálisan növekszik a lehetséges értékek száma, így könnyen végrehajthatatlanná válik.

\subsection{Tárgyterület-függő diszkretizáció}
Amennyiben az adott statisztikai változóról több információ áll rendelkezésre, készíthető speciális, az adott tárgyterülethez kapcsolódó diszkretizáció.

Ilyen lehet a laboratóriumi paraméterek \cite{katayev2010establishing} diszkretizálása, amely a jelenlegi gyakorlatot ülteti át algoritmikus formába. A mostani, erre a célra gyűjtött adatok helyett nagy mennyiségű, más mérési céllal készült statisztikai adatokkal határozza meg a referencia tartományt.

\section{Többváltozós diszkretizáció}
Az egyváltozós diszkretizáció egyszerűségéből adódik a hátránya is. A Bayes-hálóra azért van szükség, mert a változók között vannak összefüggések. Amikor egy-egy változó diszkretizációja izoláltan történik, a tartalmazott információ mennyisége csökken, és ez torzítja az összefüggéseket. Ezért a függő változók diszkretizációja során érdemes lehet figyelembe venni a kapcsolatokat, így az azokból származó információ kevésbé veszik el.

\subsection{Minimális hosszúságú leírás elve alapján}
A minimális hosszúságú leírás elve (MDL) azt mondja ki, hogy egy adathalmaz legjobb modellje az, amelyik minimalizálja a leírásához szükséges információ mennyiségét \cite{friedman1996discretizing}.

Az információ sűrűségének jellemzésére megfelelő mutató az (információ) entrópia. Így a feladatot megfogalmazható úgy is, hogy minimalizálandó a diszkretizációs elv szerint a modell entrópiája. Az algoritmus célja egy olyan reprezentációt találni az eredeti \emph{D} adatsorhoz, amely leírása bitekben minimális hosszúságú és tartalmazza a diszkretizált értékeket (részben a Bayes-hálóba kódolva), a diszkretizációs elvet, valamint a diszkretizált értékekből az eredeti adatsor visszaállításához szükséges információt.
$$ DL_{D} = DL(U^{*}, G, D^{*}) + DL_{\Lambda}(\Lambda) + DL_{D^{*} \rightarrow D}(D, \Lambda)$$
Itt a \emph{DL} a leírás hossza bitekben, \emph{D} az eredeti adatsor, \emph{G} a Bayes-háló, \emph{D*} a diszkretizált adatsor, \emph{$\Lambda$} a diszkretizációs elv és \emph{U*} a \emph{D*} egyedi értékei.

Ennek megoldása $O(n^3)$ időben futtatható, ahol az $n$ a tanító adatok száma.

\subsection{Bayes-döntés alapján}
A diszkretizációs elv kiválasztható úgy, hogy mindegyikre kiszámoljuk \textit{a posteriori} becsléssel, mennyire valószínűek a megadott adatokon, és ezek közül kiválasztjuk a legnagyobbat. A Bayes-tétel miatt ez felírható a likelihood és a prior szorzataként.
$$ \argmax_{\Lambda}{P(\Lambda | D)} =
\argmax_{\Lambda}{P(D | \Lambda) \cdot P(\Lambda)} $$
ahol $\Lambda$ a diszkretizációs elv, $D$ az adat.

Az \textit{a posteriori} becslés esetén a prior megfelelő megválasztása kulcsfontosságú. A könnyebb programozhatóság kedvéért a valószínűség helyett annak negatív logaritmáltját érdemes minimalizálni. Mivel a logaritmus szigorúan monoton növekvő függvény, ez nem változtat az eredményen, de a lebegőpontos számábrázolás miatt pontosabban számolható számítógépen.

Mivel megoldható a feladat dinamikus programozással (lehet építeni a korábban kiszámolt valószínűségekre), így az futásidő $O(n^2)$-re csökken, ahol $n$ a tanító adatok száma.

Az algoritmust a \ref{chapter:bayesdontesalapu}. fejezet részletesebben tárgyalja.

\section{Struktúra tanulás}
Egy Bayes-háló alapja egy irányított körmentes gráf. A tartalmazott változók közti függőségeket ez reprezentálja. Ezért hasznos egy adatsor alapján a függőségi hálót felépítő algoritmus, mert erre építve a Bayes-háló átmenetei már könnyen kiszámíthatók.

Általánosan két megközelítésből kiindulva működnek ezek az algoritmusok \cite{tsamardinos2006max}.

\subsection{Keresés és pontozás}
Ezek az algoritmusok a lehetséges struktúrákon haladnak, mindegyiket pontozzák az adatsorhoz illeszkedésük szerint. A pontszámokat összehasonlítják és a legjobb eredmény elért lesz a kiválasztott háló.

Ilyenek például a a \emph{pomegranate} csomagban megvalósított \emph{greedy} és \emph{exact} algoritmusok \cite{schreiber2018pomegranate}. Ezek szintén a Minimális hosszúságú leírás elve alapján döntenek, ám itt a diszkretizáció adott, a Bayes-háló struktúra változik. Emellett a struktúrától független tagokat egy \emph{büntetés} együtthatóban egyesíti, amely megváltoztatható. Így már nem a minimális hosszúságú leírással rendelkező struktúra lesz a kiválasztott, lehet több és kevesebb élt is engedélyezni.

A különbség a két említtet algoritmus között, hogy a \emph{greedy} mohó algoritmus lévén egyesével mindig a legtöbbet javító élt adja hozzá. Míg az \emph{exact} algoritmus A* keresést végez az összes háló struktúra felett.


