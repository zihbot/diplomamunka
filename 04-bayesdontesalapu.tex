%----------------------------------------------------------------------------
\chapter{Algoritmikus többváltozós diszkretizáció}
\label{chapter:bayesdontesalapu}
%----------------------------------------------------------------------------
A Diplomamunka a Bayes-döntés alapú diszkretizációs eljárást valósította meg. Ezért az alábbi fejezetben ennek részletesebb ismertetése történik. Az előző fejezetben láthattuk, hogy az algoritmus lényege a következő képlet minimalizálása:
$$ \argmin_{\Lambda}{(-\ln{P(D | \Lambda)} - \ln{P(\Lambda)})}$$
Az első tag a likelihood, a második a prior.

\section{Prior}
A tanulmányban \cite{chen2017learning} leírt algoritmus prior tagja az alábbi:
$$\sum_{i=1}^{k-1} -\ln{(1-\exp{(
-L \cdot \frac{x_{\lambda_i+1}-x_{\lambda_i}}{x_{n}-x_{1}})})} +
\sum_{i=1}^{k}
-L \cdot \frac{x_{\lambda_i}-x_{\lambda_{i-1}+1}}{x_{n}-x_{1}}$$
ahol az $x_i$ az $i$. értéke a folytonos adatsornak (növekvő sorrend esetén), $\lambda_i$ a vizsgált diszkretizációs elv $i$. tartomány végpontjának indexe, $k$ az intervallumok száma, $n$ az adatok száma és $L$ egy választható együttható, amelyhez közelíteni fog az intervallumok száma.

A tag értéke akkor lesz alacsony, amikor az intervallumok két végpontja közötti különbség a teljes adatsor hosszának $L$-ed része. Ezzel elérhető, hogy az eredmény intervallumokból nagyjából $L$ darab legyen, hiszen minél jobban eltérnének ettől a hosszaik, annál nagyobb büntetést kapna.

\section{Likelihood}
A likelihood tag az alábbi \cite{boulle2006modl}:
$$
\sum_{i = 1}^{k} \left[ \ln \binom{\gamma_i+J_P-1}{J_P-1} + \ln \frac{\gamma_i!}{n_{i,1}^{(P)}! + n_{i,2}^{(P)}! + \dotsc + n_{i,J_P}^{(P)}!}\right] +
$$
$$
\sum_{i = 1}^{k}\sum_{j=1}^{n_c} \sum_{l=1}^{J_\textbf{S}^{(j)}} \left[ \ln \binom{n_{i,l}^{(j)}+J_{C_j}-1}{J_{C_j}-1} + \ln \frac{n_{i,l}^{(j)}!} {n_{i,1,l}^{(j)}! + n_{i,2,l}^{(j)}! + \dotsc + n_{i,J_{C_j},l}^{(j)}!}  \right]
$$
ahol $\gamma_i$ az i. intervallumba tartozó adatok száma, $J_P$ a változó szülei által felvehető értékek száma (a szülők kardinalitásának szorzata), $n_{i,j}^{(P)}$ a szülő j. esetének számossága az i. intervallumban.
Valamint $n_c$ a változó gyerekeinek száma, $J_S^{j}$ a változó j. gyereke szülei felvehető értékeinek száma, $J_{C_j}$ a változó j. gyereke felvehető értékeinek száma, $n_{i,l}^{(j)}$ a j. gyerek szülei l. esetének számossága az i. intervallumban, $n_{i,r,l}^{(j)}$ pedig a j. gyerek r. esetének és szülei l. esetének számossága az i. intervallumban.

A likelihood tag lényege, hogy a változó szülei, gyerekei és azok szülei lehetséges értékeit figyelembe véve ezekből minél kevesebb típusút minél nagyobb számban tartalmazzon az intervallum. Amennyiben ez teljesül, akkor sikerült az intervallumot úgy meghatározni, hogy a tőle függő változók szerint lett csoportokra bontva.

Elég a szülő, gyerek, gyermeki szülőket vizsgálni, hiszen ez a változó Markov-takarója, amennyiben ezek a változók rögzítettek, akkor a hálóban levő többi változótól független a vizsgált változó (a Markov-takaróján keresztül feltételesen függ a többitől). Így lehet lokálisan diszkretizálni.

\section{Dinamikus programozás}
A \textit{likelihood} tagnál felismerhető, hogy minden tagja $i=1$-től $k$-ig van összeadva, tehát minden intervallumra összegezve van. Ez az összegzés kiemelhető. A $k.$ intervallumba eső összeget, amelynek határai legyenek $\alpha_{k}$ és $\beta_{k}$, jelölje $h(\alpha_{k}, \beta_{k})$. Ekkor a posterior tag $\sum_{i = 1}^{k} h(\alpha_{k}, \beta_{k})$.

Az eredeti minimalizálandó feladat szerint az intervallumok határait keressük. Az első intervallum $X_{1}$-től kell induljon, az utolsónak $X_{n}$-nél kell befejeződni. Minden (kivéve az első) intervallum kezdete az előző vége utáni elem. Ezek a kényszerek a diszkretizációs elvvel szemben támasztott követelmények.

Dinamikus programozási feladattá úgy alakítható át, hogy minden $x_{0}, x_{\psi}$ érték között megállapítjuk a legjobb diszkretizációt, és az arra adott becslést. Sorban haladva az $x$ egyedi értékein ($\delta$) megállapítható, hogy a korábbi pontok közül ($\gamma$) az $x_{0} \dots x_{\gamma}$ becslésének és a $[x_{\gamma + 1}, x_{\delta}]$ becslésének összege mikor maximális, mert ez a diszkretizáció a legvalószínűbb. Fontos, hogy a $x_{0} \dots x_{\gamma}$ nem egyetlen diszkretizációs intervallum, hanem arra már meg van állapítva a diszkretizáció, és el van tárolva az arra adott becslés. A $[x_{\gamma + 1}, x_{\delta}]$ viszont biztosan egy intervallum, hiszen ha lenne egy $\phi \mbox{ ahol } \gamma < \phi < \delta$ határpont, ami jobb értéket ad, azt a $x_{0} \dots x_{\phi}$ vizsgálatakor meg fogjuk találni. A végeredmény az $x_{0}, x_{n}$ között megtalált legjobb diszkretizáció.

Ilyen módszerrel történő keresésnél a $h$ minden két adatpont közötti értéke előre kiszámolható, és eltárolható a $H$ táblában. Ez a dinamikus programozási feladatot meggyorsítja, mert a becslés számolásánál a posterior rész kiolvasható ebből a táblából.

\section{Több változó diszkretizálása}
Ha az adatok közt több folytonos változó van, kezdetben mindegyiket L egyenlő számú adatot tartalmazó intervallumra bontjuk, tehát az \textit{Egyenlő mintaszámú intervallumok elve} szerint diszkretizáljuk,  majd iterálunk a folytonos változókon. Az algoritmus futtatása olyan adatsoron történik, amelyben a kiválasztott változó folytonos, minden más változó diszkrét vagy az aktuálisan legjobb módon diszkretizált. Így a kiválasztott folytonos változóra megkapjuk a jelenleg legmegfelelőbb diszkretizációt.

Az eredeti tanulmány szerint érdemes a gyerekektől indulva iterálni, ám a félév során ezt nem implementáltam, mivel a háló is tanulva volt, így nem csökkentette volna érdemben a feltételezéseket.

Az iterációt addig futtatjuk, ameddig a diszkretizációs elv nem konvergál.

\section{Struktúra tanulás}
A diszkretizációs eljárás a \emph{k2} algoritmust alkalmazza struktúra tanulásához. Ez az algoritmus heurisztikus kereséssel találja meg a legnagyobb valószínűségű Bayes-hálót a diszkrét adatsor alapján \cite{ruiz2005illustration}. Az algoritmus nem robusztus, bemenetként elvárja a változók sorrendjét, ez lesz az elkészült háló topológiai sorrendje.

A k2 algoritmus pontozó függvénye:
$$ f(i, \pi_{i}) =
\prod_{j = 1}^{q_{i}} \frac{(r_{i} - 1)!}{(N_{ij} + r_{i} - 1)!}
\prod_{k = 1}^{r_{i}} \alpha_{ijk}!$$
ahol a $\pi_{i}$ a vizsgált változó szülői halmaza, $\phi_{i}$ (nem szerepel a képletben, de a többi változó magyarázatához szükséges) a vizsgált változó szüleinek összes lehetséges együttesen felvehető értéke, mely a szülők felvehető értékeinek Descartes-szorzata, $q_{i}$ a $\phi_{i}$ elemszáma, $V_{i}$ a vizsgált változó lehetséges felvehető értékei, $r_{i}$ a $V_{i}$ elemszáma, $\alpha_{ijk}$ azon esetek száma, ahol a vizsgált változó a $V_{i}$ k. értékét veszi fel, szülei pedig a $\phi_{i}$ j. értékét veszik fel, $N_{ij}$ = $\sum_{k = 1}^{r_{i}}\alpha_{ijk}$ azon esetek száma, ahol a vizsgált változó szülei a $\phi_{i}$ j. értékét veszik fel.

A megadott sorrendben fut az algoritmus a változókon. Kiértékeli a vizsgált változó aktuális szüleivel az \emph{f} függvény értékét, és ezt tekinti a megtalált legjobbnak. Ezután a sorrendben a vizsgált változó előtt szereplőket lehetséges szülőnek tekinti. Kiértékeli az \emph{f} függvényt minden esetben úgy, hogy a vizsgált változó szülőhalmazához hozzáadja a szülő jelöltet. A kopott értékek közül kiválasztja a legnagyobbat, és ha ez jobb, mint a szülő jelölt nélküli érték, létrehoz egy élt a legjobb szülő jelölt és a vizsgált változó között. Ezután az új szülőhalmazhoz próbál még szülőt adni, amellyel javítható a pontszám. Ha nem talál a pontszámot javító szülőt, vagy a szülők száma eléri az arra vonatkozó korlátot (ezt alacsonyra választva a futási sebesség jelentősen növelhető), a sorrendben következő változóval folytatja.

A tanulmány úgy használja fel a k2 algoritmust, hogy először az egyenlő hosszú diszkretizációs algoritmussal kapott adatsoron futtatja. Az első él hozzáadása után a Bayes-döntés alapú algoritmust futtatja az aktuális hálón, majd az új diszkretizációval kapott adatsoron folytatja a k2 futását. Minden él hozzáadása után újabb diszkretizációt végez, ameddig a k2 algoritmus nem terminál.

A k2 algoritmus kezdeti sorrendjének meghatározására két ajánlatot tettek. Az első, hogy az egyszerű diszkretizációval rendelkező adatokon sok (1000 körüli) k2 futtatást végeztek véletlenszerű sorrendekkel, végül a kapott gráfok közül a legjobbnak a kiindulási sorrendjével futtatták a fő algoritmust. A másik, hogy kevesebb, (20-50) véletlenszerű sorrenddel futtatták a fő algoritmust, és a kapott gráfokból kiválasztották a legjobbat. A különbség, hogy a kiválasztást olyan hálókon végzik, amelyek egyszerű diszkretizáció alapján készültek, vagy már a Bayes-döntés alapján diszkretizált adatsor alapján. A második eset hosszabb futásidőt kíván.

A Diplomamunkában a \ref{chapter:kiertekelesszintetikus}. fejezetben az információelméleti alapon meghatározott sorrenddel futtatás is vizsgálva lesz.