%----------------------------------------------------------------------------
\chapter{Bayes-döntés alapján diszkretizáló csomag}\label{chapter:modul}
%----------------------------------------------------------------------------
% végére szekvenciadiagrammok, összehasonlítva az előző fejezet algoritmusával

\iffalse
A Diplomaterv részeként elkészült egy \textit{python} modul\footnote{A python nevezéktanban a függvénykönyvtárakat nevezik moduloknak.}, amely a bemutatott algoritmus használatát könnyíti meg. Az eredeti cikkben bemutatott implementáció \textit{julia} nyelven történt, amely egy gyors és dinamikusan fejlődő nyelv, melyet statisztikai feladatok megoldására gyakran használnak. Az új modul a python nyelven íródott, mert jelenleg ez a nyelv elterjedtebb, és az egyik jelentős előnye, hogy nagy mennyiségű függvénykönyvtárral rendelkezik, és ennek a bővítése segítség lehet a többi felhasználónak.

A modul célja, hogy egy robusztus többváltozós diszkretizációt hajtson végre a kapott adatsoron, valamint a kinyerhető információkat a felhasználó rendelkezésére bocsássa. A \verb|bediscretizer| név a "Bayesian Estimation based DISCRETIZER" rövidítése.

\section{Telepítés}
A modul jelenleg nem telepíthető pip-pel. Az \ref{appendix:codebase} függelékben szereplő linkről letölthető a bediscretizer könyvtár. Ezt a python HOME könyvtárának (például: \verb|C:\Python39|) \verb|Lib| alkönyvtárába bemásolva a modul telepítésre került.

\section{Használat}
Ha a modul telepítve van, a hozzá tartozó python értelmezőben futtatott \verb|import bediscretizer| parancs után felhasználhatóak a biztosított függvények.

A modul központi osztálya a \verb|MultivariateDiscretizer|. Példányosításkor meg kell adni a statisztikai adatsort egy numpy tömbként. Ezen kívül megadható az adatsor neve, amelyet később a kiírásokhoz használ.

Ezután a \verb|fit()| függvényének meghívásával lehet a tanító algoritmust futtatni. Ennek megadható, hány alkalommal futtassa a diszkretizáció és struktúra tanulási algoritmusokat, ez az epochok száma.
A futtatás után kinyerhetőek az adatok a modellből. Egy egyszerű példa látható a \ref{appendix:simpleexample} függelékben.

\section{Elérhető információk}
A \verb|MultivariateDiscretizer| osztályból az adatsorról sok adat kinyerhető. Ezeknek a listája, és a hozzájuk tartozó leírás szerepel itt.

\subsection{Tagváltozók}
\begin{itemize}
    \item[data] A megadott adatsor az előfeldolgozás után. Egy mátrix csak számokkal feltöltve.
    \item[column\_types] Az automatikusan felismert változótípus, amely lehet \verb|DISCRETE| vagy \verb|CONTINUOUS|.
    \item[discretization] A folytonos változókhoz tartozó diszkretizációs intervallumok határai.
    \item[graph] Az adatsorhoz illesztett gráf.
    \item[name] Az adatsor neve, amelyet az adatsorból kinyert adatok kijelzéséhez használ.
\end{itemize}

\subsection{Metódusok}
\begin{itemize}
    \setlength{\itemindent}{5em}
    \item[fit] A tanító algoritmus futását indítja el.
    \item[show] Az aktív \verb|pyplot figure| grafikonjára kirajzolja az illesztett gráfot.
    \item[draw\_structure\_to\_file] A paraméterként megadott fájlba menti az illesztett gráf képét.
    \item[as\_dataframe] Az adatsort \verb|pandas DataFrame| típusúként adja vissza.
\end{itemize}
\fi

Ez a fejezet bemutatja a Diplomaterv részeként elkészült bediscretizer csomag implementációját. Leírja, milyen elvárások, követelmények vannak a függvénykönyvtárral szemben. Ismerteti a mérnöki döntéseket, amelyeket a csomag létrehozásakor kellett meghozni.

A csomag python nyelven lett implementálva. Elsődleges célja, hogy egy robusztus többváltozós diszkretizációt hajtson végre a kapott adatsoron. A \verb|bediscretizer| név a "Bayesian Estimation based DISCRETIZER" rövidítése.

%% spellcheck-off
\section{Követelmények}
A csomaggal szemben támasztott követelmények a FURPS modell \cite{grady1992practical} szerint vannak csoportosítva. Ez egy angol betűszó, amely a csoportok neveinek kezdőbetűiből lett összerakva. Ezek alapján a program különböző szempontokból vizsgálható. A szempontok: funkcionalitás (\textbf{F}unctionality), használhatóság (\textbf{U}sability), megbízhatóság (\textbf{R}eliability), teljesítmény (\textbf{P}erformance) és a támogatottság (\textbf{S}upportability). Egy alkalmazás készítése során ezekre a szempontokra figyelve jobb minőségű végeredmény érhető el, mintha kizárólag a megfelelő működés a cél.
%% spellcheck-on

\subsection{Funkcionális követelmények}
A működés során az elvégzendő feladatokat, a rendszer képességeit, biztonsági kérdéseket válaszolja meg.

\begin{enumerate}
    \descitem{Bemenet} Fogadjon el numpy.ndarray típusú objektumot, amely az adatokat tartalmazza.
    \descitem{Kimenet} Legyen egy betanított Bayes-háló, amely a kapott adatok közötti kapcsolatokat reprezentálja.
    \descitem{Algoritmus} Valósítja meg a \autoref{chapter:bayesdontesalapu}. fejezetben ismertetett Bayes-döntés alapú diszkretizációs eljárást.
    \descitem{Felügyelet nélküli tanulás} Ne várjon címkézett adatokat, kizárólag a bemenet változói közötti összefüggéseket ismerje fel.
    \descitem{Automatikus adattípus} Ismerje fel a diszkrét és folytonos változókat a bemeneti adatsorban, és kezelje ennek megfelelően.
    \descitem{Konfigurálhatóság} A kezdeti konfiguráció megadható legyen, ilyenkor ne végezzen automatikus felismeréseket.
\end{enumerate}

\subsection{Használhatósági követelmények}
A felhasználói élmény javítását célozza, az emberi tényezők figyelembe vételére törekszik.

\begin{enumerate}[resume]
    \descitem{Felhasználók} A csomagot arra kell felkészíteni, hogy fejlesztők és adattudósok használhassák.
    \descitem{Bemeneti típusok} Legyen megadva a \textit{typing} modul segítségével a kezelt típusok. A python gyengén típusos nyelv, ezért ezt a nyelv nem kényszeríti ki, ám a fejlesztői környezetek felismerik és támogatják a típusokat, ezzel segítve a felhasználót.
    \descitem{Logolás} Támogassa a \textit{logging} modult. A logoló beállításait tartsa be és rögzítse a logban a folyamat lépéseit. A kezelt problémákat is jegyezze fel.
    \descitem{Robuszusság - bemenet} Ismerje fel a nem megfelelő bemenetet, és kezelje akár kivételt dobva.
    \descitem{Robuszusság - futás} Algoritmus futása közben fellépő kivételeket kezelje. A helytelen eredmény felismerése nem lehetséges, ezért ez nem is cél.
    \descitem{Ismert metódusok} Hasonlítson kiajánlott metódusok neve és működése a \textit{scikit-learn} csomag algoritmusaihoz. Ezzel segítve a felhasználók könnyebb alkalmazkodását.
    \descitem{Nyílt forráskód} Az implementáció legyen elérhető bárki számára.
\end{enumerate}

\subsection{Megbízhatósági követelmények}
A hibalehetőségek száma, gyakorisága és azok kezelésének lehetőségeit fogalmazza meg.

\begin{enumerate}[resume]
    \descitem{Belső hibák} A belső hibákat kapja el, logolja és csak saját hibákat dobjon. Ezzel elérhető, hogy a helyben kezelhető hibák ne okozzák az alkalmazás futásának a végét.
    \descitem{Determináltság} A futtatott algoritmus legyen determinisztikus
    \descitem{Diszkretizáció hiba} A diszkretizáció során fellépő hibát kezelje úgy, hogy az eredeti diszkretizációt megtartja.
\end{enumerate}

\subsection{Teljesítmény követelmények}
A futási sebességet, hatékonyságot, pontosságot határozza meg

\begin{enumerate}[resume]
    \descitem{Dinamikus programozás} Használja ki, hogy az algoritmusban használt \textit{H} mátrix értéke dinamikus programozással megoldható
    \descitem{Futási idő - referencia} A futási idő legyen a tanulmányban szereplő \textit{Julia} implementációnál rövidebb ugyanarra a bemenetre.
    \descitem{Futási idő - bemenet} A bemeneti adatpontok számának függvényében $O(n^2)$ legyen.
\end{enumerate}

\subsection{Támogatottsági követelmények}
    Tesztelhetőségi, konfigurálhatósági, bővítési lehetőségekre ír le igényeket.

\begin{enumerate}[resume]
    \descitem{Egység teszt} Az algoritmus implementációjához és a használt segédfüggvényekhez legyenek működést ellenőrző tesztek.
    \descitem{Beállítható változó típus} Legyen megadható a bemeneti változók típusa (folytonos és diszkrét)
    \descitem{Beállítható kezdeti diszkretizáció} Lehessen megadni kezdeti diszkretizációt, vagy kiválasztani az ezt létrehozó algoritmust.
\end{enumerate}

\section{Felépítés}
A python nyelven készült függvénykönyvtárakat a nevezéktan két csoportra bontja. Egyik csoport a \textbf{modul}. Ebben az esetben egyetlen fájlban szerepel a kódbázis. Ilyenkor a fájl nevével lehet rá hivatkozni, ezzel emelhető be más programokba. A modulban szereplő minden elem a névtérbe kerül és a használó programban elérhető lesz.

A másik csoport \textbf{csomag}. Ilyenkor egy könyvtárban vannak összegyűjtve a funkciók. A kiajánlott osztályokat és függvényeket a \textit{\_\_main\_\_.py} tartalmazza. Ebben lehet bármilyen python kód, de általában \textit{import} hívások sorozata, amely a csomag fájljaiból, mint modulokból gyűjti össze a kívülről elérhető nyelvi elemeket. Más programokba a könyvtár nevével emelhető be, ilyenkor a kiajánlott funkciók lesznek a névtérben. A python nem támogatja a privát nyelvi elemeket, így ismerve egy csomag felépítését, annak moduljai is beemelhetőek, de ez gyanús kódot (code smell) eredményez.

A csomag a \textit{MultivariateDiscretizer.py} modul köré épül. Megvalósítható lett volna egyetlen modulban is, de a könnyebb bővíthetőség és olvashatóság érdekében több modulra lett bontva. Így később akár más nagyobb diszkretizációs eljárások is könnyen hozzáadhatók.

A fő modul, amely az algoritmust végrehajtó osztályt tartalmazza a \textbf{MultivariateDiscretizer}. Ebben található az azonos nevű osztály, mely példányosításkor várja az adatsort, melyből automatikusan képes beállítani a konfigurációját, valamint egyéb konfigurációs paraméterek is megadhatók. Ezen az elven működnek a \textit{scikit-learn} csomag implementációi is. A tanulás megkezdéséhez a \textit{fit} metódust kell meghívni, ez szintén azonos a hasonló csomagok használatával. Azért szükséges külön bontani az inicializálást és a tanulást, mert így mérhető a sebességük külön-külön is a felhasználó számára.

A Bayes-döntés alapú diszkretizációs eljárás implementációja a \textbf{discretization} modulban szerepel. Azért szerepel külön, mert így könnyen tesztelhető fejlesztés közben, hiszen futtatható a fájl önmagában is. Emellett több különböző implementációja is elkészült az algoritmusnak, és itt ezek együtt szerepelhetnek.

Egyéb segédfüggvényeket tartalmaz a \textbf{util} modul. Ezek nem szorosan kapcsolódnak az eljáráshoz, egyéb esetekben is hasznosak lehetnek, így ki lettek szervezve egy külön modulba. Ennek az előnye, hogy könnyen írható ezekre teszteset, hiszen a bemeneti paramétereik alapján determinisztikusak. A \textit{MultivariateDiscretizer} metódusai az osztályváltozókat is használják, így ezek teszteléséhez példányosítani kell az objektumot.

Az implementált struktúra tanulási algoritmusok és ehhez segédfüggvények a \textbf{structure} modulban kaptak helyet. Emellett a \emph{pomegranate} implementációkhoz is biztosít egy csomagoló függvényt, hogy minden elérthető struktúra tanulási mód egységesen elérhető és használható legyen.

A \textbf{test} könyvtárat nem tartalmazza a csomag, de a forráskód mellett elérhető. Ezek a függvények tesztelik az implementáció működését.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{figures/app/structure.png}
    \caption{A rendszer felépítése}
    \label{fig:struktura-diagram}
\end{figure}

% TODO: tesztelési környezet hozzáadása

\section{Működés}
A csomag elsődlegesen a \emph{MultivariateDiscretizer} osztályon keresztül használható. Fő funkciói a példányosítás, a tanítás és a kiértékelés. Az folyamatok során elvégzett lépések leírását tartalmazza ez a rész.

\subsection{Példányosítás}
A használathoz először a \emph{MultivariateDiscretizer} osztályt kell példányosítani. Paraméterei között kötelező a tanító adatsort megadni ndarray típusú mátrixként. Erre azért van szükség, hogy az alapértelmezett belső paramétereket ki lehessen számolni. Ezen kívül megadható az adatsor neve, amely a generált eredmények elnevezéséhez használható, a struktúra tanuláshoz használandó algoritmus mely értéke \emph{greedy, chow-liu, exact, k2 vagy multi\_k2} lehet, alapértelmezett a multi\_k2, amely a Bayes-döntés alapú diszkretizáció eljárást futtatja a később megadott számú alkalomszor és a legjobbat kiválasztja. Felülírható az alapértelmezetten automatikusan eldöntött változó típusok, valamint a kiindulási Bayes-háló struktúrája. Kiválasztható a kezdeti diszkretizációhoz alkalmazott algoritmus, értéke \emph{equal\_width vagy equal\_sample} lehet, melyek rendre az egyenlő hosszú és az egyenlő mintaszámú eljárások.

A példányosítás során a kapott adatsorban megvizsgálja, hogy van-e olyan változó, mely értékei szövegesek. Ezek a változók diszkrétek ugyan, de a későbbi számolásokat megkönnyítik a számértékek, ezért a szöveges változó minden lehetséges értékéhez egy számot rendel, az adatsorban ezzel helyettesíti. A dekódoláshoz az objektum \emph{column\_unique\_values} attribútumában megmaradnak a hozzárendelések, ez bármikor elérhető.

Amennyiben a változók típusa nincsen megadva, automatikusan felismeri a típusokat. Ehhez a változókon egyesével ellenőrzi, hogy a változó összes értéke egész szám-e. Amennyiben igen, az osztályban definiált \emph{ColumnType} enum \emph{DISCRETE} értékét állítja be rá, ha nem, akkor pedig a \emph{CONTINUOUS} értéket. Az így kapott lista a \emph{column\_types} attribútumban érhető el. Hasonló formában kell megadni a példányosításkor, ha nem az automatikus felismerés a cél.

Ezután a beállított kezdeti diszkretizációt végzi el. Először a diszkrét változókon végig haladva mindegyik kardinalitását megállapítja. Ezt a numpy \emph{unique} \footnote{https://numpy.org/doc/stable/reference/generated/numpy.unique.html} függvény segítségével teszi, mely visszaadja az egyedi értékeket a változóban. Ezek száma pontosan a kardinalitás. A legnagyobb számosságot eltárolja a \emph{number\_of\_classes} attribútumban.

A folytonos változókon végig haladva a beállított kezdeti diszkretizáció alapján meghatározza a diszkretizációs elvet, majd a \emph{discretization} attribútum megfelelő elemeként eltárolja. Ez az attribútum egy lista, mely minden változóhoz tartalmazza a rajta használandó diszkretizációs elvet, ami a vágási értékék listája. A diszkrét változóknál a diszkretizációs elv \emph{None}.

Az egyenlő mintaszámú diszkretizáció során a kiszámolja, mely sorszámú elemek lesznek a határpontok. Ezek az \emph{adatok száma} / \emph{number\_of\_classes} hányados egészre kerekített értéke helyeken állnak. A változó rendezett értékeinek listájából kiemeli ezeket az elemeket, listába rendezi, ez lesz a diszkretizációs elv az adott változóra.

Az egyenlő hosszú intervallumok esetében a változó értékeinek minimumát és maximumát keresi ki. A \emph{min + (max - min)} / \emph{number\_of\_classes} értékek adják ebben az esetben a diszkretizációs elvet.

A kezdeti diszkretizáció meghatározása után, amennyiben nincsen kezdeti háló struktúra megadva, a beállított struktúra tanuló algoritmussal, valamint a diszkretizációs elv által meghatározott diszkrét adatsorral fut a struktúra tanulás. Ezt azért végzi el, hogy legyen egy kiindulási struktúra. A kezdeti struktúra a \emph{graph} attribútumban van eltárolva, \emph{networkx.DiGraph} \footnote{https://networkx.org/documentation/stable/reference/classes/digraph.html} típusú objektumként.

\subsection{Tanítás}
A példányosítás után a kitöltött attribútumokat meg lehet vizsgálni, felül lehet írni, ezért a tanítás külön lépésben szerepel. Emellett ha nem szükséges a diszkretizáció - struktúra tanulás lépések ismételgetése, (például egyváltozós diszkretizációt kell csak alkalmazni,) a példányosítás után rendelkezésre is áll a betanított Bayes-háló, nem szükséges semmilyen tanító lépést meghívni.

A tanítás a \emph{fit} függvény meghívásával indítható. Paraméterként a maximális futtatások számát várja. Ennek jelentése algoritmusonként eltér, de arra szolgál, hogy a meghatározott számú iteráció után mindenképp leálljon, ne kerüljön végtelen ciklusba. A meghívás után a beállított struktúra tanuló algoritmus alapján folytatódik a futás.

A \emph{k2} beállítás egy teljes k2-es tanítást futtat a \ref{chapter:bayesdontesalapu}. fejezetben leírt algoritmus alapján. Ez elindítható a \emph{fit\_k2} metódussal is, mely paraméterként a változók sorrendjét várja. Ennek megadására a \emph{fit} függvény esetében nincsen lehetőség. Ebben az esetben véletlenszerű sorrenden történik a futtatás. A struktúra tanulásához a \emph{structure} modul \emph{learn\_structure} függvényét \emph{p\_step} paramétert megadva hívja fel, ezért az a k2 algoritmust a paraméter értékét felhasználva folytatja, és egy új él megtalálása után azonnal visszatér az új p\_step-pel. Így érhető el, hogy azonos interfészen megvalósulhasson a struktúrának részleges és teljes tanulása is.

A struktúra tanulás után meghívja a belső \emph{\_discretize\_all} metódust, mely eltárolja a jelenlegi diszkretizációt. Ezután a jelenlegi gráf topológiai sorrendjében visszafele haladva minden folytonos változóra meghívja a \emph{\_discretize\_one} metódust, mely frissíti annak a változónak a diszkretizációs elvét. Majd miután az összes frissült, ellenőrzi, hogy változtak-e az eltárolt eredetihez képest. Amig történik változás, de legfeljebb 10 alaklommal, ezt a három lépést ismétli.

Az adott változó diszkretizációs elvének frissítéséhez a \emph{util} modul \emph{largest\_markov\_cardinality}, majd az ebből kapott kardinalitással a \emph{discretization} modul \emph{discretize\_one} függvényének felhívása történik. Erről részletesebben a modulok leírásánál lesz szó.

A k2 tanítás a struktúra tanulás egy lépését, majd teljes diszkretizációt futtat, ameddig a struktúrába kerül új él. Ezután a kialakult modellt eltárolja a \emph{final\_model} attribútumban, és a tanítás visszatér.

A \emph{multi\_k2} algoritmussal ellenőrzi, hogy a változók permutációinak száma meghaladja-e a megadott maximális futtatások számát. Amennyiben nem, minden permutációban futtatja az előzőekben ismertetett k2 tanítást, másként a lehetséges sorrendekből kiválasztja a megengedett számút, és ezekkel futtatja a k2 tanítást.

Minden sorrendű futás után értékeli a felépített hálót a \emph{util.preference\_bias\_full} függvénnyel, és az ez alapján legjobbnak ítéltet a végén újra betanítja. Mivel a k2 tanítás meghatározott sorrend mellett determinisztikus, ugyanaz lesz az eredmény, mint ami a legjobb gráfot létrehozta. Az újra tanítás azért szükséges, mert így elegendő a változó sorrendet tárolni, nem kell minden más információt is.

A \emph{chow-liu, greedy, exact} paraméterek esetén két metódus is implementálva van, de ezek nem adnak vissza értelmezhető eredményt. Az egyik a \emph{\_fit\_basic\_structure\_learner}, amely a Friedman és Goldszmidt \cite{friedman1996discretizing} által meghatározott algoritmus szerint tanítja a hálót. Ez alapján egymás után futtatja a Bayes-döntés alapú diszkretizációt és a megadott struktúra tanulást addig, amíg a diszkretizáció nem konvergál. Ez a struktúra konvergenciával egyszerre történik meg, mert ez a diszkretizációs algoritmus determinisztikus. A probléma az volt, hogy a struktúra tanuló algoritmusok olyan hálót készítettek, amely alapján a diszkretizáció gyakran üres halmazt talált csak megfelelőnek, tehát a változók minden értékét ugyanabba az osztályba sorolta. A tanulmányban megadott Julia-ban készített referencia implementáció is ugyanígy viselkedett ezeken a struktúrákon, úgyhogy ez nem implementációs hiba.

A másik metódus a \emph{\_fit\_basic\_structure\_after\_node\_added}, mely a k2 tanulást utánozza a beépített struktúra készítőkkel. Nyilvántart egy külön gráfot, mely az elején üres. Minden struktúra tanítási lépésben az algoritmusnak megadja kényszerként, hogy ezt a gráfot tartalmaznia kell. Az algoritmus által készített háló minden élét, amellyel még DAG marad, hozzá próbálja adni a nyilvántartott gráfhoz. Ezeket pontozza a multi\_k2-ben megismert módon, és a legjobbat megtartja, így mindig eggyel növelve a gráf éleinek számát. Ezután végzi el a diszkretizációt, egészen addig, míg a struktúra tanuló algoritmus nem növeli az élek számát. Ezt futtatva ugyanaz lett az eredmény, mint a \_fit\_basic\_structure\_learner esetén, mert ezek a struktúra tanuló algoritmusok nem voltak érzékenyek a diszkretizációra, ugyanazt a gráfot adták vissza minden esetben. Ez az Iris adatsoron történt, lehet, hogy más adatsorral jobban működnének.

A \emph{best\_edge} nevet algoritmusnak megadva szintén a k2 tanulás egy variánsa fut. Ebben az esetben nem a meghatározott sorrend alapján megy végig a változókon, hanem az összes lehetséges élt próbálja beilleszteni a nyilvántartott gráfba, majd a multi\_k2-ben megismert módon pontozottakból a legmagasabb pontszámot elérőt illeszti be végül. Véletlenszerű sorrenden futtatott k2 algoritmusnál nem ért el jobb eredményt az Iris adathalmazon. Emellett fennáll a veszélye, hogy nagy adathalmaznál a lehetséges élek nagy száma miatt jelentősen lelassul.

\subsection{Kiértékelés}
A kiértékelés során a modell tesztelése történik. A tanítás után egy tesztelő adatsorral (amely lehetőség szerint olyan adatpontokat tartalmaz, melyek a tanító adatsornak nem voltak részei) predikciót készít a modell. A predikált értékek pedig összehasonlíthatók a valódi értékekkel. Az összehasonlításra különböző módszerke vannak. A Bayes-hálók esetében, mivel diszkrét adatokkal dolgoznak, a klasszifikációs kiértékelési módszerek alkalmazhatóak, tehát a predikció az adatpontokról megállapítja, hogy a célváltozó melyik osztályába tartoznak, a kiértékelés pedig konfúziós mátrix alapján történik.

A predikcióhoz a \emph{MultivariateDiscretizer predict} metódusa alkalmazható. Ennek paraméterként a teszt adatsor kell megadni, valamint a célváltozó azonosítóját. Ezután a tárolt diszkretizációs elv szerint a teszt adatokat is diszkretizálja, ismeretlenre állítja a célváltozó értékeket, majd ezt az adatsort a tárolt betanított modell \emph{predict} \footnote{\url{https://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html\#pomegranate.BayesianNetwork.BayesianNetwork.predict}} metódusának átadja. Az eredményből visszaadja a célváltozóra kapott predikált értékeket.

A konfúziós mátrix elkészítésére az \emph{evaluate} metódus szolgál. Ennek bemenetként a valós és a predikált értékekre van szüksége, emellett megadható a korábbi kiértékelések eredménye is. Ha mag van adva, akkor a korábbiak végéhez fűzi a számolt értékeket, ha nincsen akkor egy új eredmény objektumot hoz létre. Többosztályos változó esetén minden változóra kiszámolja a scikit-learn \emph{confusion\_matrix} \footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion\_matrix.html} függvénye által kiadott konfúziós mátrixból a változó bináris konfúziós mátrixát, ennek értékeit teszi az eredmény objektumba. A kiértékelés metrikái bináris konfúziós mátrixon működnek, ezért van szükség az átalakításra.
%https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal
%TODO: konfúziós mátrix az elejére

A metrikák alkalmazásához az \emph{evaluation\_summary} metódus használható. Ez az előzőekben elkészített eredmény objektumot várja, annak értékeit összegzi, így megkapja hogy az összes teszt során összesen hány darab valódi pozitív (TP), hamis pozitív (FP), valódi negatív (TN) és hamis negatív (FN) eredmény lett. A metódus kulcs-érték párokként visszaadja a k2 tanításával foglalkozó tanulmányban \cite{aghdam2019some} szereplő metrikák eredményeit. Ezek a valódi pozitív, hamis pozitív, valódi pozitív arány, hamis felfedezési arány, pozitív prediktív érték, pontosság, Matthews korrelációs együttható és az F-pontszám. Ezek mindegyike kiszámolható a kapott négy adatból, és összehasonlíthatóvá teszi a Bayes-hálókat.

Valódi pozitív arány $TPR = \frac{TP}{TP + FN}$, hamis felfedezési arány $= \frac{FP}{FP + TP}$,
pozitív prediktív érték $PPV = \frac{TP}{TP+FP}$, pontosság $= \frac{TP + TN}{TP + FP + TN + FN}$,
Matthews korrelációs együttható $= \frac{TP \cdot TN-FP \cdot FN}{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}$,
F-pontszám $= 2 \cdot \frac{PPV \cdot TPR}{PPV + TPR}$.

%TODO: szekvencia diagram

\section{Modulok}
Ez a rész tartalmazza MultivariateDiscretizer melletti modulok implementációinak leírását. Ezekben nincsen osztály definiálva, csak kiajánlott függvények, amelyek a csomag elsődleges funkcióján kívül is használhatók, rendezett csoportokban.

\subsection{Util}
A \emph{discretize} függvény az adatsorból és a diszkretizációs elvből elkészíti a diszkretizált adatsort. Ehhez a DataFrame-ként kapott adatsorban egy segédoszlopot hoz létre. A diszkretizációs elvvel rendelkező oszlopokon megy végig, az elv minden határánál nagyobb értékű elemeket a határpont sorszámának megfelelő osztályba helyez a segédoszlopban, majd ezt visszamásolja az eredeti helyére.

A \emph{bn\_to\_graph} a pomegranate.BayesianNetwork típusú modellből készít DiGraph típusú gráfot. A modell struktúrája minden változóhoz tartalmazza a szülőváltozóit, ezen iterálva felveszi az éleket az új gráfba.

A \emph{graph\_to\_bn\_structure} az előző inverze, az adott gráf struktúrából készíti el azt a modellt, melyet a pomegranate struktúra tanuló algoritmusai értelmezni tudnak.

A \emph{parents\_to\_graph} a k2 algoritmus által készített listából, mely minden változóhoz a szüleit tartalmazza, készíti el a gráfot bn\_to\_graph-hoz hasonlóan.

A \emph{show} függvény a Bayes-hálót rajzolja ki. Átalakítja gráffá, amit a networkx csomag gráfrajzoló függvénye jelenít meg.

A \emph{concat\_array} két numpy tömböt egyesít oly módon, hogy ha valamelyik dimenzióban egyforma darabszámú elemet tartalmaznak, annak mentén illeszti az adatsort. Az adatsorokban gyakran külön változóban vannak a tanító és a célváltozók, ezek illesztéséhez hasznos.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{figures/app/markov-blanket.png}
    \caption{A D változó Markov-takarója}
    \label{fig:markov-blanket}
\end{figure}

A \emph{markov\_blanket} egy gráf és egy változó azonosítója alapján kikeresi a változó Markov-takarójába tartozó változókat. Ebbe a vizsgált változó szülei, gyerekei és gyerekeinek a más szülei tartoznak bele. Ezek rögzített értéke mellett a változó független az összes többi változótól. Egy példa a \ref{fig:markov-blanket}. ábrán látható.

A \emph{largest\_markov\_cardinality} végigmegy a kapott változó Markov-takaróján, és kiválasztja a legnagyobb kardinalitású számosságát.

A \emph{preference\_bias} a k2 algoritmust pontozó algoritmus. A pontszámot ez az \emph{f} függvény adja, ami a \ref{chapter:bayesdontesalapu}. fejezetben ki van fejtve:
$$ f(i, \pi_{i}) =
\prod_{j = 1}^{q_{i}} \frac{(r_{i} - 1)!}{(N_{ij} + r_{i} - 1)!}
\prod_{k = 1}^{r_{i}} \alpha_{ijk}!$$
Amennyiben a változó szülőinek száma 0, akkor létrehoz egy olyan szülőt, melynek minden értéke egyforma. Ez a képlet alkotójának is az elképzelése \cite{cooper1992bayesian}. Az implementáció a numerikus stabilitás érdekében a függvény logaritmáltját számolja, így a szorzatból összeg lesz. Ez az eredményeken nem változtat, mivel a természetes alapú logaritmus szigorúan monoton növekvő, és a kapott pontszámok csak egymással vannak összehasonlítva. A faktoriális logaritmusa emellett átírható $\Gamma$ függvényes alakba.
$$ ln(n!) = ln(\Gamma(n + 1))$$
Ennek segítségével a scikit-learn \emph{gammaln} \footnote{https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.gammaln.html} függvénye is használható a pontszám kiszámítására. Ez pedig a numpy mátrixokon sokkal nagyobb sebességgel képes a műveletet elvégezni, mint egy elemenkénti megvalósítás \cite{smith2015cython}, mert a háttérben futó \emph{C} nyelven írt kódban történik az iteráció. Ezért az implementációban nagyon fontos szerepet kapott, hogy minden műveletet a numpy és scikit-learn függvények végezzenek, a függvény kódjában a lehető legkevesebb \emph{for} ciklus szerepeljen.

A \emph{preference\_bias\_full} a teljes gráfnak határozza meg az értékét a k2 pontozó algoritmusa alapján. Minden változóra meghívja az előző pontozást, az eredményeket pedig összeadja. Azért elegendő ez a művelet, mert a pontszám logaritmáltját adja vissza. Ez az összeg a gráf értéke.

Az \emph{entropy} egy diszkrét adatsor változója Shannon entrópiájának kiszámolására szolgál. Ennek értéke:
$$ H(\mathcal{X}) = -\sum_{i = 1}^{n} p(x_{i}) \cdot log(p(x_{i}))$$
tehát a diszkrét változó értékének előfordulási valószínűségei és logaritmáltjaik szorzatának összege. Az implementáció természetes alapú logartimust használ, az eredmények összehasonlításra szolgálnak, így bármilyen egynél nagyobb logaritmus alap megfelelő. A sebesség itt is lényeges, így numpy függvényekkel vannak a számítások megoldva. Együttes entrópia számolására is képes, amennyiben tömb helyett mátrixot kap bemenetként.

A \emph{relative\_entropy} egy diszkrét adatsor több változója relatív Shannon entrópiájának kiszámolására szolgál. Ez megkapható:
$$ H(\mathcal{Y} | \mathcal{X}) = -\sum_{i = 1}^{n}\sum_{j = 1}^{m} p(x_{i}, y_{y}) \cdot log(\frac{p(x_{i}, y_{y})}{p(x_{i})})
= H(\mathcal{Y}, \mathcal{X}) - H(\mathcal{X})$$
Az implementációban az előző függvény meghívása történik először az együttes entrópia kiszámolására, majd a feltétel entrópiájának számolására. Ezért a feltételben itt is lehetséges több változó szerepeltetése.

A \emph{mutual\_information} a kölcsönös információt számolja ki az egyik változó entrópiája és a másikkal vett feltételes entrópia különbségeként. AZ ezt használó algoritmus vagy lassan fut vagy nem determinisztikus, ezért ez nem volt használva.

\subsection{Structure}
A \emph{learn\_structure} egységesíti a pomegranate beépített struktúra tanuló algoritmusai, valamint a k2 algoritmus futtatását. A k2 esetén a struktúra tanítás utána a pomegranate \emph{from\_structure} \footnote{\url{https://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html\#pomegranate.BayesianNetwork.BayesianNetwork.from\_structure}} algoritmussal készíti el a Bayes-hálót. Így minden esetben az elkészült és betanított Bayes-hálóval tér vissza, de egyedi struktúra tanulást is lehetővé tesz.

A \emph{k2\_order\_mutual\_information} és a \emph{k2\_order\_entropies} egyaránt Aghdam et al. \cite{aghdam2019some} cikkében szereplő algoritmusok implementációja. Ezek információelméleti alapon keresik ki a legmegfelelőbb, k2 tanuláshoz szükséges topológiai sorrendet. Az első esetében a kezdő változó az eredeti leírásban nincsen meghatározva, az implementációban a legkisebb entrópiájú változó. Ezután a legutóbb hozzáadott változóval vett legnagyobb együttes információval rendelkező változót adja a sorrend végéhez. Mivel a kezdő változó nem meghatározott, az algoritmus használatára nem került sor. A második függvény relatív entrópiák szerinti sorrend eredménye a \ref{chapter:kiertekelesszintetikus}. és \ref{chapter:kiertekelesvalos}. fejezetekben megjelenik. Ez az eredeti algoritmus szerint is a legkisebb entrópiájú változóval kezd, majd mindig azt adja a sorrend végéhez, amelyikhez a feltéve a korábbi változókat a feltételes entrópia a legalacsonyabb. \emph{A, B, C} változók esetében az algoritmusok így futnak:

A kölcsönös információ alapúnál legyen B-ből indítva. Ekkor az $MI(B, A)$ és az $MI(B, C)$ közül a nagyobbat választjuk, most legyen ez az $MI(B, A)$. Az $MI$ a kölcsönös információ meghatározására szolgáló függvény. Ekkor az $A$-t adjuk a sorhoz, és az $MI(A, C)$ közül kell választani. Mivel ez egyedül van, így a $C$ is hozzáadódik, a végső sorrend \emph{B, A, C}.

A feltételes entrópia alapúnál keressük a $H(A)$, $H(B)$ és $H(C)$ közül a legkisebbet. Ez legyen a $H(C)$, így a $C$ a sorrend első eleme. Ezután a $H(A|C)$ és $H(B|C)$ közül kell a legkisebb. Ez legyen a $H(A|C)$, így az $A$ a következő elem. Végül a $H(B|A,C)$ közül a legkisebb az egyetlen, így $B$ az utolsó elem. A végső sorrend \emph{C, A, B}.

A \emph{learn\_k2\_structure} a k2 algoritmus implementációja. Annyi bővítés szerepel benne a \ref{chapter:bayesdontesalapu}. fejezetben ismertetetthez képest, hogy megadható a p\_step paraméter, amely az aktuális gráfállapotot tartalmazza. Ilyenkor ebből az állapotból folytatódik az algoritmus futása és új él hozzáadása után azonnal visszatér.

\subsection{Discretization}
Ez a modul tartalmazza a Bayes-döntés alapú diszkretizációs algoritmust. A MultivariateDiscretizer által használt függvény a \emph{discretize\_one}. Ez bemenetként megkapja a teljes diszkrét adatsort, a modell hálóját, a vizsgált változó folytonos adatsorát, valamint a célt az intervallumok számára (\emph{L}). A dinamikus programozással előre kiszámolható \emph{H} táblát (mely az ismertetett $h$ értékeket tartalmazza minden adatpont között) elkészítő függvényt felhívja. Erre 3 különböző implementáció is készült, amely a későbbiekben lesz bemutatva. A prior tag kiszámolása numpy tömbműveletekkel történik, mert is lényeges a futási sebesség. Ezután a tanulmányban leírt módon történik az összegzés és a diszkretizációs határok keresése a prior és a posterior tag alapján.

A \emph{H} tábla kiszámítására az első függvény a \emph{precalculate\_probability\_table\_as\_definition}. Ez a \emph{H} táblát úgy állítja elő, hogy végigmegy a folytonos értékekből képzett párokon, és képlet alapján mindegyikre kiszámolja az eredményt, majd ezeket eltárolja. Ez nagyon lassú folyamat, mert minden párnál a köztük lévő adatokat ki kell olvasni az adatsorból és processzor intenzív számításokat végezni (logaritmus, faktoriális).

Ennél jobban működik a \emph{precalculate\_probability\_table\_split\_up}. Ez a szülők, valamint a gyermekek és gyermeki szülők felvett értékeinek darabszámát minden \emph{x} érték esetében egy külön táblába gyűjti. Minden párnál az eggyel kisebb intervallumra számolt darabszámokhoz hozzáadja az újhoz tartozó darabszámot, és ezt is eltárolja. Így minden párhoz megkapja a köztük lévő intervallumon felvett értékek darabszámait viszonylag kevés számítással. A \emph{h} kiértékelésekor elég az átmeneti táblából kiolvasni a felvett értékek darabszámát, nem kell minden egyes párnál újra kiszámolni.

Az implementáltak közül a \emph{precalculate\_probability\_table\_split\_up\_numpy} a legjobban optimalizált algoritmus. Ez a $h$ kiértékelését nem \emph{for} ciklusokkal végzi, hanem a numpy és scikit-learn függvényeit használja. Az implementáció az előző függvény alapján készült, minden \emph{for} ciklusnál a benne szereplő függvényeknek meg kellett találni a numpy implementációját, az átadott tenzorokat megfelelő formára hozni, és minden lépést külön elvégezni.

Az eredmények az eredeti tanulmányhoz mellékelt \emph{data\_auto\_mpg.csv} alapján voltak értékelve. Ennek az 5. oszlopán történt a vizsgálat, melynek a példa teszt szerint a 3. oszlop a szülője, az 1. és a 7. pedig a gyereke. A definíció szerinti $H$ számítással a diszkretizáció \textbf{3 perc 9.717 másodpercig}  tartott. Ugyanez a külön táblában tárolónál \textbf{2 perc 32.736 másodpercet vett igénybe}. Végül a numpy optimalizációval készült $H$ táblával \textbf{3.664 másodperc} kellett a diszkretizációhoz. A julia-ban implementált referencia algoritmusnak ugyanehhez \textbf{14.543 másodperc}re volt szüksége, mert ott nem áll rendelkezésre hasonló optimalizálási lehetőség. Ez egyetlen változó egyetlen diszkretizációja, a teljes algoritmus futása ennél hosszabb, de az arányok ott is megmaradnak. Ennek oka, hogy a $H$ tábla előállítása a folyamat legidőigényesebb része. Azért az 5. változó volt vizsgálva, mert ez rendelkezik szülővel és gyerekkel is, amely a kódlefedettséget biztosítja.

\section{Tesztelés}
A tesztek a \emph{test} könyvtárban találhatók, a \emph{unittest} modult felhasználva készültek. A \emph{test.py} tartalmazza a MultivariateDiscretizer-t és a diszkretizációt ellenőrző teszteket. Ezek megállapítják, hogy megfelelően tölti-e be az adatokat, valamint a példa adathalmazon elvégzett diszkretizáció helyes-e. A \emph{unit\_test.py} ellenőrzi, hogy az util diszkretizációja megfelelően működik, valamint a Markov-takarót is meg tudja határozni.

A tesztek futtatásához a gyökérkönyvtárban kell kiadni a követkető parancsot:

\begin{lstlisting}
    python -m unittest test.unit_test
    python -m unittest test.test
\end{lstlisting}

Ez meghívja az unittest modult a test könyvtárban található fájlokra, és futtatja a teszteket.